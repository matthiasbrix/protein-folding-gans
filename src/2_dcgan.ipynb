{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "# from torchsummary import summary ONLY works on pip!\n",
    "\n",
    "# CODE FILES HERE\n",
    "from model_params import get_model_data_dcgan\n",
    "from models.dcgan import Dcgan, Generator, Discriminator\n",
    "from models.resnet import ResNet, ResGenNet, ResDiscNet\n",
    "\n",
    "from solver import Solver, Testing\n",
    "from directories import Directories\n",
    "from dataloader import DataLoader\n",
    "from plots import plot_losses, plot_z_samples, plot_grid\n",
    "from sampling import dcgan_sampling, generate_latent_points, interpolate_points, find_closest_gt\n",
    "from contact_maps import get_contact_maps\n",
    "import preprocessing\n",
    "\n",
    "# SETTINGS HERE\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" # to see the CUDA stack\n",
    "%matplotlib inline\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# supress cluttering warnings in solutions\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "# Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing from raw files to actual hd5 files\n",
    "dataset_arg = \"proteins\"\n",
    "prefix = \"_max_length/\"\n",
    "data_root_folder = \"../data/proteins/\"\n",
    "data = get_model_data_dcgan(dataset_arg)\n",
    "\n",
    "residue_fragments = 256\n",
    "preprocessing.MAX_SEQUENCE_LENGTH = 256\n",
    "\n",
    "preprocessing.process_raw_data(False, force_pre_processing_overwrite=False, prefix=prefix,\n",
    "                               data_root_folder=data_root_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_name = \"training_50\"\n",
    "training_file = data_root_folder+ train_file_name + \".txt.hdf5\"\n",
    "print(\"training file: {0}\".format(training_file))\n",
    "padding = \"pwd_pad\"\n",
    "print(\"padding: {}\".format(padding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = Directories(\"dcgan\", dataset_arg.lower(), data[\"z_dim\"], make_dirs=False)\n",
    "data_loader = DataLoader(directories, data[\"batch_size\"], dataset_arg.lower(),\n",
    "                         training_file=training_file, residue_fragments=residue_fragments,\n",
    "                         atom=\"calpha\", padding=padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show ground truth contact maps\n",
    "samples = get_contact_maps(training_file, fragment_length=residue_fragments, padding=padding).unsqueeze(1)\n",
    "print(samples.shape)\n",
    "samples = samples[0:25]\n",
    "plot_grid(samples, None, nrow=5, ncol=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train dcgan model\n",
    "#model = Dcgan(data_loader.input_dim, data[\"z_dim\"])\n",
    "#generator = Generator(data[\"z_dim\"], res=residue_fragments)\n",
    "#discriminator = Discriminator(1, 1, res=residue_fragments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train resnet model only for 256 fragments!\n",
    "model = ResNet(data_loader.input_dim, data[\"z_dim\"])\n",
    "num_layers = 5\n",
    "generator = ResGenNet(data[\"z_dim\"], num_layers)\n",
    "discriminator = ResDiscNet(num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(generator, (100, 1, 1))\n",
    "# summary(discriminator, (1, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = Solver(model, generator, discriminator, data[\"epochs\"], data_loader, data[\"optimizer_G\"],\n",
    "                data[\"optimizer_D\"], data[\"optim_config_G\"], data[\"optim_config_D\"],\n",
    "                preprocessing.MAX_SEQUENCE_LENGTH, data[\"one_sided_labeling\"], data[\"g_updates\"],\n",
    "                save_model_state=False)\n",
    "solver.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert name of model here if want to load a model\n",
    "LOAD_MODEL = 1\n",
    "if LOAD_MODEL:\n",
    "    res_dir = \"../results/\"\n",
    "    load_file = res_dir+\"2000_64_17_850\"\n",
    "    solver = torch.load(load_file+\".pt\", map_location=\"cpu\")\n",
    "    generator = solver.generator\n",
    "    discriminator = solver.discriminator\n",
    "    solver.data_loader.directories.make_dirs = False\n",
    "else:\n",
    "    res_dir = solver.data_loader.directories.result_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing file: ../data/proteins/preprocessed/test_proteins.pt\n",
      "Reading cache file in ../data/proteins/preprocessed/test_proteins_64_no_pad_contact_maps.dat\n",
      "Testing complexity of the GAN, steps: 3000\n"
     ]
    }
   ],
   "source": [
    "# test complexity of model as in A.4\n",
    "test_file_name = \"test_proteins\"\n",
    "prefix = \"_max_length/\"\n",
    "data_root_folder = \"../data/proteins/\"\n",
    "PDB = 1\n",
    "if not PDB:\n",
    "    testing_file = data_root_folder+\"preprocessed/\" + str(preprocessing.MAX_SEQUENCE_LENGTH)\\\n",
    "                 + prefix + test_file_name + \".txt.hdf5\"\n",
    "else:\n",
    "    testing_file = data_root_folder+\"preprocessed/\" + test_file_name +\".pt\"\n",
    "\n",
    "    print(\"testing file: {0}\".format(testing_file))\n",
    "optim_config_G = {\n",
    "    \"lr\": 1e-2,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"betas\": (0.5, 0.999)\n",
    "}\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), **optim_config_G)\n",
    "test_loader = solver.data_loader.get_new_test_data_loader(testing_file=testing_file, batch_size=1, padding=\"no_pad\",\\\n",
    "                                                          test_pdb=True)\n",
    "testing = Testing(solver.generator, solver.model.z_dim, optimizer_G, test_loader, load_file+\"_test_generator_dict.pt\")\n",
    "testing.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load inference generator\n",
    "# generator = Generator(solver.model.z_dim, res=solver.data_loader.residue_fragments)\n",
    "# generator.load_state_dict(torch.load(load_file+\"_test_generator_dict.pt\"))\n",
    "generator.eval()\n",
    "discriminator.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting g and d losses for all epochs\n",
    "plot_losses(solver, solver.train_loss_history[\"g_loss\"], solver.train_loss_history[\"d_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling a grid of distance maps\n",
    "samples = dcgan_sampling(generator, solver.model.z_dim, 25).detach().numpy()\n",
    "imgs, rows, cols = solver.get_sample_stats()\n",
    "plot_grid(samples[:imgs], res_dir+\"/plot_grid.png\", nrow=rows, ncol=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del imgs\n",
    "del samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate points in latent space\n",
    "pts = generate_latent_points(solver.model.z_dim, 2)\n",
    "# interpolate points in latent space\n",
    "zs = interpolate_points(pts[0], pts[1])\n",
    "samples = generator(zs).detach().numpy()\n",
    "imgs, rows, cols = solver.get_sample_stats()\n",
    "plot_grid(samples[:imgs], res_dir+\"/plot_linear_interpolation.png\", 1, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pts\n",
    "del zs\n",
    "del samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the ground truth data set\n",
    "max_len = 256\n",
    "train_file_name = \"training_95\"\n",
    "training_file = \"../data/proteins/\"+\"preprocessed/\" + str(max_len)\\\n",
    "                + \"_max_length/\" + train_file_name + \".txt.hdf5\"\n",
    "print(training_file)\n",
    "test_loader = solver.data_loader.get_new_test_data_loader(testing_file=training_file, batch_size=1)\n",
    "# find closest ground truth\n",
    "min_loss_maps = find_closest_gt(generator, solver.model.z_dim, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, rows, cols = solver.get_sample_stats()\n",
    "gens = torch.FloatTensor([x[2] for x in min_loss_maps[:imgs]])\n",
    "gt_maps = [x[3] for x in min_loss_maps[:imgs]]\n",
    "gt_maps = torch.cat(gt_maps).unsqueeze(1)\n",
    "losses = [x[1] for x in min_loss_maps[:imgs]]\n",
    "print(\"Losses: {}\".format(losses))\n",
    "l2_losses = [x[1] for x in min_loss_maps]\n",
    "print(\"L2 map error {} {}\".format(sum(l2_losses), sum(l2_losses)/len(l2_losses)))\n",
    "asd = torch.cat([gens, gt_maps])\n",
    "plot_grid(asd, res_dir+\"/_plot_\" + sum(l2_losses) + \"_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test_loader\n",
    "del min_loss_maps\n",
    "del gens\n",
    "del gt_maps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openprotein",
   "language": "python",
   "name": "openprotein"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
