{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/matthias/DIKU Project Protein Folding/protein-folding-gans/src', '/home/matthias/.local/share/virtualenvs/openprotein-xj-YAKcS/lib/python37.zip', '/home/matthias/.local/share/virtualenvs/openprotein-xj-YAKcS/lib/python3.7', '/home/matthias/.local/share/virtualenvs/openprotein-xj-YAKcS/lib/python3.7/lib-dynload', '/home/matthias/anaconda3/envs/openprotein/lib/python3.7', '', '/home/matthias/.local/share/virtualenvs/openprotein-xj-YAKcS/lib/python3.7/site-packages', '/home/matthias/.local/share/virtualenvs/openprotein-xj-YAKcS/lib/python3.7/site-packages/IPython/extensions', '/home/matthias/.ipython', '/home/matthias/DIKU Project Protein Folding/protein-folding-gans/src/openprotein/pnerf']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(os.sys.path)\n",
    "os.sys.path.append(\"/home/matthias/DIKU Project Protein Folding/protein-folding-gans/src/openprotein/pnerf\")\n",
    "\n",
    "# CODE FILES HERE\n",
    "from model_params import get_model_data_dcgan\n",
    "from models.dcgan import Dcgan, Generator, Discriminator\n",
    "from solver import Solver, Testing\n",
    "from directories import Directories\n",
    "from dataloader import DataLoader\n",
    "from plots import plot_losses, plot_z_samples, contact_map_grid\n",
    "from sampling import dcgan_sampling\n",
    "from contact_maps import get_contact_maps\n",
    "import preprocessing\n",
    "\n",
    "# SETTINGS HERE\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" # to see the CUDA stack\n",
    "%matplotlib inline\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# supress cluttering warnings in solutions\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "# Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pre-processing of raw data...\n",
      "files ['../data/proteins/raw/training_50.txt', '../data/proteins/raw/training_95.txt', '../data/proteins/raw/training_100.txt', '../data/proteins/raw/validation', '../data/proteins/raw/testing.txt', '../data/proteins/raw/sample.txt', '../data/proteins/raw/training_70.txt', '../data/proteins/raw/training_90.txt', '../data/proteins/raw/training_30.txt']\n",
      "Preprocessed file for training_50.txt already exists.\n",
      "Skipping pre-processing for this file...\n",
      "Preprocessed file for training_95.txt already exists.\n",
      "Skipping pre-processing for this file...\n",
      "Preprocessed file for training_100.txt already exists.\n",
      "Skipping pre-processing for this file...\n",
      "Preprocessed file for validation already exists.\n",
      "Skipping pre-processing for this file...\n",
      "Preprocessed file for testing.txt already exists.\n",
      "Skipping pre-processing for this file...\n",
      "Preprocessed file for sample.txt already exists.\n",
      "Skipping pre-processing for this file...\n",
      "Preprocessed file for training_70.txt already exists.\n",
      "Skipping pre-processing for this file...\n",
      "Preprocessed file for training_90.txt already exists.\n",
      "Skipping pre-processing for this file...\n",
      "Preprocessed file for training_30.txt already exists.\n",
      "Skipping pre-processing for this file...\n",
      "Completed pre-processing.\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing from raw files to actual hd5 files\n",
    "dataset_arg = \"proteins\"\n",
    "prefix = \"_max_length/\"\n",
    "data_root_folder = \"../data/proteins/\"\n",
    "data = get_model_data_dcgan(dataset_arg)\n",
    "\n",
    "residue_fragments = 256\n",
    "preprocessing.MAX_SEQUENCE_LENGTH = 256\n",
    "\n",
    "preprocessing.process_raw_data(False, force_pre_processing_overwrite=False, prefix=prefix,\n",
    "                               data_root_folder=data_root_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training file: ../data/proteins/preprocessed/256_max_length/training_50.txt.hdf5\n"
     ]
    }
   ],
   "source": [
    "train_file_name = \"training_50\"\n",
    "training_file = data_root_folder+\"preprocessed/\" + str(preprocessing.MAX_SEQUENCE_LENGTH)\\\n",
    "            + prefix + train_file_name + \".txt.hdf5\"\n",
    "print(\"training file: {0}\".format(training_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the contact maps for ../data/proteins/preprocessed/256_max_length/training_50.txt.hdf5 as no cache was found!\n"
     ]
    }
   ],
   "source": [
    "directories = Directories(\"dcgan\", dataset_arg.lower(), data[\"z_dim\"], make_dirs=True)\n",
    "data_loader = DataLoader(directories, data[\"batch_size\"], dataset_arg.lower(),\n",
    "                         training_file=training_file, residue_fragments=residue_fragments, atom=\"calpha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = get_contact_maps(training_file, fragment_length=residue_fragments).unsqueeze(1)\n",
    "sample = sample[:25]\n",
    "contact_map_grid(sample, rows=5, cols=5, fill=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train model\n",
    "dcgan = Dcgan(data_loader.input_dim, data[\"z_dim\"])\n",
    "generator = Generator(data[\"z_dim\"], res=residue_fragments)\n",
    "discriminator = Discriminator(1, 1, res=residue_fragments)\n",
    "solver = Solver(dcgan, generator, discriminator, data[\"epochs\"], data_loader, data[\"optimizer_G\"],\n",
    "                data[\"optimizer_D\"], data[\"optim_config_G\"], data[\"optim_config_D\"],\n",
    "                preprocessing.MAX_SEQUENCE_LENGTH, save_model_state=False)\n",
    "solver.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_name = \"testing\"\n",
    "testing_file = data_root_folder+\"preprocessed/\" + str(preprocessing.MAX_SEQUENCE_LENGTH)\\\n",
    "             + prefix + test_file_name + \".txt.hdf5\"\n",
    "print(\"testing file: {0}\".format(testing_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test complexity of model as in A.4\n",
    "optim_config_G = {\n",
    "    \"lr\": 1e-2,\n",
    "    \"weight_decay\": None,\n",
    "    \"betas\": (0.5, 0.999)\n",
    "}\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), **optim_config_G)\n",
    "testing = Testing(solver)\n",
    "test_loader = solver.data_loader.get_new_test_data_loader(testing_file=testing_file)\n",
    "testing.test(optimizer_G, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert name of model here if want to load a model\n",
    "LOAD_MODEL = 0\n",
    "if LOAD_MODEL:\n",
    "    res_dir = \"\" # \"../../results/proteins_z=100_0/\"\n",
    "    solver = torch.load(res_dir+\"model_state.pt\", map_location=\"cpu\")\n",
    "    generator = solver.generator\n",
    "    discriminator = solver.discriminator\n",
    "    generator.eval()\n",
    "    discriminator.eval()\n",
    "    solver.data_loader.directories.make_dirs = False\n",
    "else:\n",
    "    res_dir = solver.data_loader.directories.result_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting g and d losses for all epochs\n",
    "plot_losses(solver, solver.train_loss_history[\"g_loss\"], solver.train_loss_history[\"d_loss\"])exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = dcgan_sampling(generator, solver.model.z_dim, 25).detach().numpy()\n",
    "imgs, rows, cols = solver.get_sample_stats()\n",
    "contact_map_grid(samples[:imgs], rows=rows, cols=cols, fill=True, file_name=res_dir+\"/plot_grid.png\", show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openprotein",
   "language": "python",
   "name": "openprotein"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
