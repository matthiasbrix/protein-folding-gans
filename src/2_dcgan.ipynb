{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/matthias/DIKU Project Protein Folding/protein-folding-gans/src', '/home/matthias/.local/share/virtualenvs/openprotein-xj-YAKcS/lib/python37.zip', '/home/matthias/.local/share/virtualenvs/openprotein-xj-YAKcS/lib/python3.7', '/home/matthias/.local/share/virtualenvs/openprotein-xj-YAKcS/lib/python3.7/lib-dynload', '/home/matthias/anaconda3/envs/openprotein/lib/python3.7', '', '/home/matthias/.local/share/virtualenvs/openprotein-xj-YAKcS/lib/python3.7/site-packages', '/home/matthias/.local/share/virtualenvs/openprotein-xj-YAKcS/lib/python3.7/site-packages/IPython/extensions', '/home/matthias/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(os.sys.path)\n",
    "os.sys.path.append(\"/home/matthias/DIKU Project Protein Folding/protein-folding-gans/src/openprotein/pnerf\")\n",
    "\n",
    "# CODE FILES HERE\n",
    "from model_params import get_model_data_dcgan\n",
    "from models.dcgan import Dcgan, Generator, Discriminator\n",
    "from solver import Solver\n",
    "from directories import Directories\n",
    "from dataloader import DataLoader\n",
    "from plots import plot_losses, plot_z_samples, plot_z_grid\n",
    "from sampling import dcgan_sampling\n",
    "from openprotein.preprocessing import process_raw_data\n",
    "import openprotein.preprocessing\n",
    "\n",
    "# SETTINGS HERE\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" # to see the CUDA stack\n",
    "%matplotlib inline\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# supress cluttering warnings in solutions\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "# Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 16\n",
      "Starting pre-processing of raw data...\n",
      "files ['openprotein/data/raw/training_50.txt', 'openprotein/data/raw/training_95.txt', 'openprotein/data/raw/training_100.txt', 'openprotein/data/raw/validation', 'openprotein/data/raw/testing.txt', 'openprotein/data/raw/sample.txt', 'openprotein/data/raw/training_70.txt', 'openprotein/data/raw/training_90.txt', 'openprotein/data/raw/training_30.txt']\n",
      "Preprocessed file for training_50.txt already exists.\n",
      "Skipping pre-processing for this file...\n",
      "Preprocessed file for training_95.txt already exists.\n",
      "Skipping pre-processing for this file...\n",
      "Preprocessed file for training_100.txt already exists.\n",
      "Skipping pre-processing for this file...\n",
      "Preprocessed file for validation already exists.\n",
      "Skipping pre-processing for this file...\n",
      "Preprocessed file for testing.txt already exists.\n",
      "Skipping pre-processing for this file...\n",
      "Preprocessed file for sample.txt already exists.\n",
      "Skipping pre-processing for this file...\n",
      "Preprocessed file for training_70.txt already exists.\n",
      "Skipping pre-processing for this file...\n",
      "Preprocessed file for training_90.txt already exists.\n",
      "Skipping pre-processing for this file...\n",
      "Preprocessed file for training_30.txt already exists.\n",
      "Skipping pre-processing for this file...\n",
      "Completed pre-processing.\n"
     ]
    }
   ],
   "source": [
    "dataset_arg = \"proteins\"\n",
    "prefix = \"_max_length/\"\n",
    "data = get_model_data_dcgan(dataset_arg)\n",
    "residue_fragments = 16\n",
    "openprotein.preprocessing.MAX_SEQUENCE_LENGTH = 128\n",
    "print(openprotein.preprocessing.MAX_SEQUENCE_LENGTH, residue_fragments)\n",
    "process_raw_data(False, force_pre_processing_overwrite=False, prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training file: openprotein/data/preprocessed/128_max_length/testing.txt.hdf5\n",
      "validation file: openprotein/data/preprocessed/128_max_length/testing.txt.hdf5\n"
     ]
    }
   ],
   "source": [
    "train_file_name = \"testing\"\n",
    "val_file_name = train_file_name\n",
    "training_file = \"openprotein/data/preprocessed/\" + str(openprotein.preprocessing.MAX_SEQUENCE_LENGTH)\\\n",
    "            + prefix + train_file_name + \".txt.hdf5\"\n",
    "validation_file = \"openprotein/data/preprocessed/\" + str(openprotein.preprocessing.MAX_SEQUENCE_LENGTH)\\\n",
    "            + prefix + val_file_name + \".txt.hdf5\"\n",
    "print(\"training file: {0}\\nvalidation file: {1}\".format(training_file, validation_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading cache file in openprotein/data/preprocessed/128_max_length/testing_contact_maps.dat\n",
      "Reading cache file in openprotein/data/preprocessed/128_max_length/testing_contact_maps.dat\n",
      "(16, 16) 256 3 3 82 3 82\n"
     ]
    }
   ],
   "source": [
    "directories = Directories(\"dcgan\", dataset_arg.lower(), data[\"z_dim\"], make_dirs=True)\n",
    "data_loader = DataLoader(directories, data[\"batch_size\"], dataset_arg.lower(),\n",
    "                         training_file=training_file, validation_file=validation_file,\n",
    "                         residue_fragments=residue_fragments, atom=\"calpha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++ START RUN | saved files in dcgan/proteins_z=100_0 +++++\n",
      "params used:\n",
      " epochs: 50\n",
      "dim(z): 100\n",
      "batch_size: 32\n",
      "optimizer_G: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.5, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0.0\n",
      ")\n",
      "optimizer_D: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.5, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0.0\n",
      ")\n",
      "dataset: proteins\n",
      "img dims: (16, 16)\n",
      "atom: calpha\n",
      "Dcgan(\n",
      "  (loss): BCELoss()\n",
      ")Generator(\n",
      "  (layers): Sequential(\n",
      "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1))\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): LeakyReLU(negative_slope=0.2)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): LeakyReLU(negative_slope=0.2)\n",
      "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): LeakyReLU(negative_slope=0.2)\n",
      "    (12): ConvTranspose2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      ")Discriminator(\n",
      "  (layers): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2)\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): LeakyReLU(negative_slope=0.2)\n",
      "    (6): Dropout(p=0.1, inplace=False)\n",
      "    (7): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): LeakyReLU(negative_slope=0.2)\n",
      "    (10): Dropout(p=0.1, inplace=False)\n",
      "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): LeakyReLU(negative_slope=0.2)\n",
      "    (14): Dropout(p=0.1, inplace=False)\n",
      "    (15): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1))\n",
      "    (16): Sigmoid()\n",
      "  )\n",
      ")\n",
      "====> Epoch: 1 g_loss / d_loss avg: 1.0587 / 0.4098\n",
      "79.55 seconds for epoch 1\n",
      "====> Epoch: 2 g_loss / d_loss avg: 1.6137 / 0.2047\n",
      "78.88 seconds for epoch 2\n",
      "====> Epoch: 3 g_loss / d_loss avg: 2.2357 / 0.1059\n",
      "72.57 seconds for epoch 3\n",
      "====> Epoch: 4 g_loss / d_loss avg: 2.5722 / 0.0758\n",
      "66.25 seconds for epoch 4\n",
      "====> Epoch: 5 g_loss / d_loss avg: 2.7085 / 0.0783\n",
      "64.91 seconds for epoch 5\n"
     ]
    }
   ],
   "source": [
    "dcgan = Dcgan(data_loader.input_dim, data[\"z_dim\"])\n",
    "generator = Generator(data[\"z_dim\"], data_loader.input_dim, data_loader.img_dims, res=residue_fragments)\n",
    "discriminator = Discriminator(1, 1, res=residue_fragments)\n",
    "solver = Solver(dcgan, generator, discriminator, data[\"epochs\"], data_loader, data[\"optimizer_G\"],\n",
    "                data[\"optimizer_D\"], data[\"optim_config_G\"], data[\"optim_config_D\"], save_model_state=False)\n",
    "solver.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert name of model here if want to load a model\n",
    "#res_dir = \"../../results/proteins_z=100_0/\"\n",
    "#solver = torch.load(res_dir+\"model_state.pt\", map_location=\"cpu\")\n",
    "#solver.generator.eval()\n",
    "#solver.discriminator.eval()\n",
    "#solver.data_loader.directories.make_dirs = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting g and d losses for all epochs\n",
    "plot_losses(solver, solver.train_loss_history[\"g_loss\"], solver.train_loss_history[\"d_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = dcgan_sampling(generator, solver.model.z_dim, solver.data_loader.img_dims, 25).detach().numpy()\n",
    "imgs, rows, cols = solver.get_sample_stats()\n",
    "plot_z_grid(samples[:imgs], res_dir+\"grid.png\", rows=rows, cols=cols, fill=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#import torchvision\n",
    "#asd = torch.FloatTensor(np.load(\"openprotein/data/preprocessed/2000_max_length/testing_contact_maps.dat\", allow_pickle=True))\n",
    "#test = asd\n",
    "#for i in range(10):\n",
    "#    plt.axis(\"off\")\n",
    "#    plt.imshow(test[i])\n",
    "#    fig1 = plt.gcf()\n",
    "#    plt.draw()\n",
    "#    fig1.savefig('tessstttyyy'+str(i)+'.png', dpi=100, bbox_inches=\"tight\", pad_inches=0.0, transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openprotein",
   "language": "python",
   "name": "openprotein"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
